{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a toy language translator\n",
    "\n",
    "We will use the **Tab-delimited Bilingual Sentence Pairs** you can get a particular data set from here: http://www.manythings.org/anki/\n",
    "\n",
    "#### How the data looks:\n",
    "\n",
    "English + TAB + The Other Language\n",
    "\n",
    "Tom broke the window.\tトムは窓を割った。<br>\n",
    "Tom checked the time.\tトムは時間を確認した。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 60 \n",
    "latent_dim = 200  # Latent dimensionality of the encoding space for the characters\n",
    "num_samples = 20000 # number of pairs for training (English sentence, Spanish sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and processing sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/translation/spa-eng/spa.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the lines of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "targets = []\n",
    "input_chars = set()\n",
    "target_chars = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "\n",
    "## Last line is blank: remove    \n",
    "lines = lines[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 118,121 \n"
     ]
    }
   ],
   "source": [
    "print(\"Number of examples: {:,} \".format(len(lines)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting (x,y) pairs\n",
    "\n",
    "- For each line: extract inputs and targets\n",
    "- For the target sequence indicate the begining of the sequence with a TAB (\\t) and the *end of sequence* with a NEW LINE (\\n).\n",
    "- Build the sets of unique characters for input and target sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in lines[: min(num_samples, len(lines))]:\n",
    "    x_seq, y_seq = line.split('\\t')\n",
    "    y_seq = '\\t' + y_seq + '\\n'\n",
    "\n",
    "    inputs.append(x_seq); targets.append(y_seq)\n",
    "    \n",
    "    for char in x_seq:\n",
    "        if char not in input_chars:\n",
    "            input_chars.add(char)\n",
    "            \n",
    "    for char in y_seq:\n",
    "        if char not in target_chars:\n",
    "            target_chars.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique input tokens: 73\n",
      "Number of unique output tokens: 89\n",
      "Max sequence length for inputs: 20\n",
      "Max sequence length for outputs: 70\n"
     ]
    }
   ],
   "source": [
    "input_chars = sorted(list(input_chars))\n",
    "target_chars = sorted(list(target_chars))\n",
    "num_encoder_tokens = len(input_chars)\n",
    "num_decoder_tokens = len(target_chars)\n",
    "max_encoder_seq_length = max([len(txt) for txt in inputs])\n",
    "max_decoder_seq_length = max([len(txt) for txt in targets])\n",
    "\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping each character to an integer index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_chars)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_chars)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building input data\n",
    "\n",
    "Each input sentece will be represented by a matrix of dimensions: `max_encoder_seq_length x num_encoder_tokens` of ones and zeros. 1 in (i,j) if character i is token j, 0 otherwise.\n",
    "\n",
    "`decoder_target_data` will be ahead of `decoder_input_data` by one timestep and will not include the start character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder inputs\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(inputs), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "# Decoder inputs\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(inputs), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "# Decoder targets\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(inputs), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (x_seq, y_seq) in enumerate(zip(inputs, targets)):\n",
    "    for t, char in enumerate(x_seq):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    for t, char in enumerate(y_seq):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "## Obtain probabilities for each token\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complete model: chain encoder and decoder, compile and train\n",
    "\n",
    "This model will transform the encoder_inputs and decoder_inputs in decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/60\n",
      "18000/18000 [==============================] - 36s 2ms/step - loss: 0.7044 - val_loss: 0.7373\n",
      "Epoch 2/60\n",
      "18000/18000 [==============================] - 34s 2ms/step - loss: 0.5451 - val_loss: 0.6327\n",
      "Epoch 3/60\n",
      "18000/18000 [==============================] - 33s 2ms/step - loss: 0.4891 - val_loss: 0.5902\n",
      "Epoch 4/60\n",
      "18000/18000 [==============================] - 33s 2ms/step - loss: 0.4497 - val_loss: 0.5437\n",
      "Epoch 5/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.4192 - val_loss: 0.5122\n",
      "Epoch 6/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.3960 - val_loss: 0.4941\n",
      "Epoch 7/60\n",
      "18000/18000 [==============================] - 34s 2ms/step - loss: 0.3768 - val_loss: 0.4766\n",
      "Epoch 8/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.3600 - val_loss: 0.4583\n",
      "Epoch 9/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.3453 - val_loss: 0.4498\n",
      "Epoch 10/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.3324 - val_loss: 0.4436\n",
      "Epoch 11/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.3210 - val_loss: 0.4404\n",
      "Epoch 12/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.3104 - val_loss: 0.4191\n",
      "Epoch 13/60\n",
      "18000/18000 [==============================] - 36s 2ms/step - loss: 0.3009 - val_loss: 0.4208\n",
      "Epoch 14/60\n",
      "18000/18000 [==============================] - 36s 2ms/step - loss: 0.2920 - val_loss: 0.4076\n",
      "Epoch 15/60\n",
      "18000/18000 [==============================] - 36s 2ms/step - loss: 0.2839 - val_loss: 0.4091\n",
      "Epoch 16/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.2762 - val_loss: 0.4058\n",
      "Epoch 17/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.2691 - val_loss: 0.3971\n",
      "Epoch 18/60\n",
      "18000/18000 [==============================] - 36s 2ms/step - loss: 0.2623 - val_loss: 0.4021\n",
      "Epoch 19/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.2561 - val_loss: 0.3972\n",
      "Epoch 20/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.2501 - val_loss: 0.3923\n",
      "Epoch 21/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.2446 - val_loss: 0.3936\n",
      "Epoch 22/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.2392 - val_loss: 0.3900\n",
      "Epoch 23/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.2342 - val_loss: 0.3905\n",
      "Epoch 24/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.2293 - val_loss: 0.3915\n",
      "Epoch 25/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.2246 - val_loss: 0.3908\n",
      "Epoch 26/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.2203 - val_loss: 0.3916\n",
      "Epoch 27/60\n",
      "18000/18000 [==============================] - 36s 2ms/step - loss: 0.2158 - val_loss: 0.3914\n",
      "Epoch 28/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.2119 - val_loss: 0.3890\n",
      "Epoch 29/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.2080 - val_loss: 0.3929\n",
      "Epoch 30/60\n",
      "18000/18000 [==============================] - 36s 2ms/step - loss: 0.2040 - val_loss: 0.3981\n",
      "Epoch 31/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.2003 - val_loss: 0.3972\n",
      "Epoch 32/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.1968 - val_loss: 0.3944\n",
      "Epoch 33/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.1934 - val_loss: 0.4012\n",
      "Epoch 34/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.1900 - val_loss: 0.3998\n",
      "Epoch 35/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.1869 - val_loss: 0.3994\n",
      "Epoch 36/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.1836 - val_loss: 0.4068\n",
      "Epoch 37/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.1804 - val_loss: 0.4006\n",
      "Epoch 38/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.1774 - val_loss: 0.4126\n",
      "Epoch 39/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.1745 - val_loss: 0.4115\n",
      "Epoch 40/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.1717 - val_loss: 0.4185\n",
      "Epoch 41/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.1690 - val_loss: 0.4158\n",
      "Epoch 42/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.1662 - val_loss: 0.4201\n",
      "Epoch 43/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.1636 - val_loss: 0.4231\n",
      "Epoch 44/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.1611 - val_loss: 0.4247\n",
      "Epoch 45/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.1589 - val_loss: 0.4273\n",
      "Epoch 46/60\n",
      "18000/18000 [==============================] - 34s 2ms/step - loss: 0.1564 - val_loss: 0.4337\n",
      "Epoch 47/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.1539 - val_loss: 0.4327\n",
      "Epoch 48/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.1516 - val_loss: 0.4325\n",
      "Epoch 49/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.1494 - val_loss: 0.4376\n",
      "Epoch 50/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.1471 - val_loss: 0.4385\n",
      "Epoch 51/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.1449 - val_loss: 0.4445\n",
      "Epoch 52/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.1431 - val_loss: 0.4490\n",
      "Epoch 53/60\n",
      "18000/18000 [==============================] - 34s 2ms/step - loss: 0.1410 - val_loss: 0.4529\n",
      "Epoch 54/60\n",
      "18000/18000 [==============================] - 34s 2ms/step - loss: 0.1391 - val_loss: 0.4582\n",
      "Epoch 55/60\n",
      "18000/18000 [==============================] - 34s 2ms/step - loss: 0.1370 - val_loss: 0.4575\n",
      "Epoch 56/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.1352 - val_loss: 0.4632\n",
      "Epoch 57/60\n",
      "18000/18000 [==============================] - 34s 2ms/step - loss: 0.1334 - val_loss: 0.4673\n",
      "Epoch 58/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.1315 - val_loss: 0.4644\n",
      "Epoch 59/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.1298 - val_loss: 0.4739\n",
      "Epoch 60/60\n",
      "18000/18000 [==============================] - 35s 2ms/step - loss: 0.1281 - val_loss: 0.4796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\direc\\Anaconda3\\envs\\recipes-ai\\lib\\site-packages\\keras\\engine\\topology.py:2368: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 200) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 200) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.1)\n",
    "# Save model\n",
    "model.save('./SavedModels/seq2seq.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder part:\n",
    "\n",
    "`encoder_model` transforms `encoder_inputs` into `encoder_states`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder part:\n",
    "\n",
    "`decoder_model` transforms (decoder_inputs, decoder_states_inputs) into  (decoder_outputs, decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reverse mapping: (index: char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geting predictions:\n",
    "\n",
    "The translated sequences will be produced character by character, following these steps:\n",
    "\n",
    "For the first character:\n",
    "1. Produce the zero-matrix representing the input sentence\n",
    "1. For each caracter in input sentence, fill with 1 in the corresponding place\n",
    "1. Get the initial state from the encoder\n",
    "1. Generate an empty target sequence of length 1\n",
    "1. Populate the first character of target sequence with the start character\n",
    "\n",
    "For the following output characters, enter for following loop: for each character, while `stop_condition = False`, start with an empty decoded sequence.\n",
    "\n",
    "1. Feed `target_seq` and `states_value` to get a prediction from `decoder_model`\n",
    "1. Extract the character with the highest probability from `output_tokens` \n",
    "1. Add this character to the decoded sentence\n",
    "1. Check for exit condition: either hit max length or find stop character.\n",
    "1. Reset the target sequence for next character\n",
    "1. Update states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sentence(input_sentence):\n",
    "    #Produce the zero-matrix representing the input sentence\n",
    "    input_seq = np.zeros((1, max_encoder_seq_length, num_encoder_tokens))\n",
    "    #For each caracter in input sentence, fill with 1 in the corresponding place\n",
    "    for t, char in enumerate(input_sentence):\n",
    "        input_seq[0, t, input_token_index[char]] = 1.\n",
    "    #Get the initial state from the encoder\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate an empty target sequence of length 1\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        #Feed target_seq and states_value to get a prediction from decoder_model\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        #Extract the character with the highest probability from output_tokens\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        #Add this character to the decoded sentence\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Check for exit condition\n",
    "        if (sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "        # Reset the target sequence for next character\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wait. : Espere.\n",
      "\n",
      "This is yours. : Esto es el momento.\n",
      "\n",
      "Are you OK? : ¿Eres normal?\n",
      "\n",
      "I am here. : Estoy aquí.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = ['Wait.', 'This is yours.', 'Are you OK?', 'I am here.']\n",
    "for sentence in sentences:\n",
    "    print(sentence, ':', decode_sentence(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a very bad translator :(\n",
    "\n",
    "Some reasons:\n",
    "\n",
    "1. Incomplete dataset\n",
    "1. Very small dataset\n",
    "1. Character-level model\n",
    "1. In a real translator this would be just a piece of a larger system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribution: <br>\n",
    "Most of the code based on:\n",
    "\n",
    "https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
