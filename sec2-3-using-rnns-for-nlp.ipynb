{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Recurrent Neural Networks for Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import re, string\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Producing Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('data/Reviews.csv', usecols=['Score', 'Text'])\n",
    "reviews.rename(columns=lambda x: x.lower(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.\n"
     ]
    }
   ],
   "source": [
    "print(reviews['text'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Some pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get rid of punctuations and upper case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>great cereal  a great price  taste good even t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>put it on anything love it on anything thats p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>these are excellent chocolates just dont let t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>good price fast shipping good instructions wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>i am obsessed with researching stuff and i was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>i bought this cappuccino because of the priceb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>these are great take a long time to ship and r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>i have been buying these for years from walmar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>this was my favorite comfort food as a child a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>its like a traditional mexican cocoa but insta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                                               text\n",
       "0      5  great cereal  a great price  taste good even t...\n",
       "1      5  put it on anything love it on anything thats p...\n",
       "2      3  these are excellent chocolates just dont let t...\n",
       "3      5  good price fast shipping good instructions wit...\n",
       "4      4  i am obsessed with researching stuff and i was...\n",
       "5      5  i bought this cappuccino because of the priceb...\n",
       "6      5  these are great take a long time to ship and r...\n",
       "7      5  i have been buying these for years from walmar...\n",
       "8      4  this was my favorite comfort food as a child a...\n",
       "9      5  its like a traditional mexican cocoa but insta..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "reviews['text'] = reviews['text'].apply(lambda x: regex.sub('', x).lower())\n",
    "## Shuffle the reviews\n",
    "reviews = reviews.sample(frac=1, random_state=777).reset_index(drop=True)\n",
    "reviews.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping only first 50 words of the reviews\n",
    "\n",
    "We are using only the first 50 words in every review, (sequences of length 50), those reviews with less that 50 words will be padded with vectors of zeros and longer reviews will be truncated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 50\n",
    "reviews['sequence'] = reviews['text'].apply(lambda x: x.split()[:max_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-label reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27e84a4de48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reviews.loc[reviews['score']==1,'score'] = 2\n",
    "reviews.loc[reviews['score']==3,'score'] = 4\n",
    "reviews['score'].value_counts(sort=False).plot(kind='barh');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    0.144316\n",
       "4    0.216895\n",
       "5    0.638789\n",
       "Name: score, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews['score'].value_counts(sort=False, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Getting training and testing subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(reviews['sequence'].values,\n",
    "                                                    pd.get_dummies(reviews['score']).values, # one-hot encoding\n",
    "                                                    test_size=0.05, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Producing word embedings\n",
    "\n",
    "Important parameters\n",
    "\n",
    "1. Vocabulary size\n",
    "1. Window\n",
    "1. Embedding dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the gensim library:\n",
    "\n",
    "`conda install -c anaconda gensim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\direc\\Anaconda3\\envs\\recipes-ai\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125068487, 226945120)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_vocabulary_size = 3000\n",
    "embedding_dim = 50\n",
    "min_count = 5\n",
    "\n",
    "model = Word2Vec(size=embedding_dim,\n",
    "                 window=6,\n",
    "                 min_count=min_count,\n",
    "                 max_vocab_size=max_vocabulary_size)\n",
    "model.build_vocab(X_train)\n",
    "model.train(sentences=X_train, total_examples=model.corpus_count, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.91803992, -2.44036102, -3.49612331, -0.92471123,  1.0321672 ,\n",
       "        2.66399598, -1.93754029,  4.74564362,  1.00914228,  0.95863259,\n",
       "        1.31733918, -1.59876704,  0.81256729,  1.33300054,  3.31886506,\n",
       "        0.69455218, -2.00803733,  1.76593316,  3.29307652, -2.06365609,\n",
       "        0.36898309, -0.47447032, -2.41572547,  2.77821445, -4.18435001,\n",
       "        0.58786094, -0.55224586,  1.18403113,  0.60682547,  1.05824172,\n",
       "        4.43394995,  1.09848392, -3.27739215,  2.98259044,  1.46257973,\n",
       "        1.64573169,  0.09466935, -0.8062306 , -2.01446009,  2.55797005,\n",
       "        0.10935304,  3.00712371, -0.80702281,  3.06787491,  1.88474381,\n",
       "       -0.41363949, -3.91767955, -4.73285341, -1.53697217, -4.57130527], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['product']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fantastic', 0.8739721775054932),\n",
       " ('wonderful', 0.8125402927398682),\n",
       " ('awesome', 0.7705838084220886),\n",
       " ('perfect', 0.7591169476509094),\n",
       " ('good', 0.7415000200271606),\n",
       " ('amazing', 0.6989656686782837),\n",
       " ('excellent', 0.696956992149353),\n",
       " ('nice', 0.6343750357627869),\n",
       " ('reasonable', 0.6248853206634521),\n",
       " ('delicious', 0.596721351146698)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['great'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('keeps', 0.39703893661499023),\n",
       " ('makes', 0.38137316703796387),\n",
       " ('without', 0.3775547742843628),\n",
       " ('takes', 0.3626345098018646),\n",
       " ('youre', 0.3588273525238037),\n",
       " ('everyday', 0.3581927418708801),\n",
       " ('keep', 0.35073983669281006),\n",
       " ('kids', 0.32740432024002075),\n",
       " ('meals', 0.31569379568099976),\n",
       " ('veggies', 0.3051527142524719)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(negative=['disappointed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ok', 0.7417405843734741),\n",
       " ('terrible', 0.7382642030715942),\n",
       " ('good', 0.6568938493728638),\n",
       " ('bland', 0.5921519994735718),\n",
       " ('weak', 0.5737581849098206),\n",
       " ('fine', 0.5590717792510986),\n",
       " ('cheap', 0.5342099070549011),\n",
       " ('maybe', 0.5301104784011841),\n",
       " ('bitter', 0.5171574950218201),\n",
       " ('salty', 0.4717325270175934)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['bad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words considered in the model:  944\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of words considered in the model: \", len(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare embedings for modelling\n",
    "\n",
    "Going from a sequence of words to a sequence of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_array_train = np.zeros(shape=(X_train.shape[0], max_words, embedding_dim))\n",
    "for i, review in enumerate(X_train):\n",
    "    for j, word in enumerate(review):\n",
    "        if word in model.wv.vocab:\n",
    "            embedding_array_train[i, j] = model.wv[word]\n",
    "            \n",
    "embedding_array_train = embedding_array_train.astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_array_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.31250000e+00,   4.17578125e+00,  -2.99414062e+00, ...,\n",
       "          1.66796875e+00,  -3.60107422e-01,  -1.73535156e+00],\n",
       "       [  3.77929688e+00,  -5.30078125e+00,   2.96484375e+00, ...,\n",
       "          1.16406250e+00,   3.01757812e-01,  -3.02343750e+00],\n",
       "       [ -1.36816406e+00,   2.95898438e+00,  -1.72265625e+00, ...,\n",
       "         -1.00781250e+00,  -1.05175781e+00,  -9.92187500e-01],\n",
       "       ..., \n",
       "       [ -3.32031250e-01,  -5.81250000e+00,  -1.59960938e+00, ...,\n",
       "         -1.37207031e+00,   1.89062500e+00,   4.02587891e-01],\n",
       "       [ -3.42968750e+00,  -1.47070312e+00,   1.49154663e-03, ...,\n",
       "          8.66699219e-01,   3.00585938e+00,   2.04492188e+00],\n",
       "       [  2.60351562e+00,  -3.63671875e+00,   1.17578125e+00, ...,\n",
       "          2.93457031e-01,  -5.39550781e-02,  -1.06750488e-01]], dtype=float16)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_array_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_array_test = np.zeros(shape=(X_test.shape[0], max_words, embedding_dim))\n",
    "for i, review in enumerate(X_test):\n",
    "    for j, word in enumerate(review):\n",
    "        if word in model.wv.vocab:\n",
    "            embedding_array_test[i, j] = model.wv[word]\n",
    "            \n",
    "embedding_array_test = embedding_array_test.astype(np.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Predicting type of review using LSTM Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Building the input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "X_train_ph = tf.placeholder(tf.float16, shape=embedding_array_train.shape)\n",
    "y_train_ph = tf.placeholder(tf.int32, shape=y_train.shape)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_ph, y_train_ph))\n",
    "train_dataset = train_dataset.apply(tf.contrib.data.batch_and_drop_remainder(batch_size))\n",
    "iterator = train_dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create placeholders to pass values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Placeholders\n",
    "n_categories = y_train.shape[1] ## n_categories=3\n",
    "\n",
    "X = tf.placeholder(tf.float16, shape=[None, max_words, embedding_dim])\n",
    "y = tf.placeholder(tf.int32, shape=[None, n_categories])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating a LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN(X):\n",
    "    units_in_cells = [200, 32]\n",
    "    rnn_layers = [tf.nn.rnn_cell.LSTMCell(size, activation=tf.nn.tanh, use_peepholes=True) for size in units_in_cells]\n",
    "    multi_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(rnn_layers)\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(multi_rnn_cell, X, dtype=tf.float16)\n",
    "    ## Additional dense layers before getting the logits\n",
    "    dense_1 = tf.layers.dense(inputs=rnn_outputs[:, -1, :], units=32, activation=tf.nn.relu)\n",
    "    dense_2 = tf.layers.dense(inputs=dense_1, units=16, activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(inputs=dense_2, units=n_categories, activation=None)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Getting the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probabilities:\n",
    "logits = RNN(X)\n",
    "y_pred_prob = tf.nn.softmax(logits)\n",
    "# Predicted category:\n",
    "y_pred = tf.cast(tf.argmax(y_pred_prob, axis=1), tf.int32)\n",
    "y_labels = tf.argmax(y, axis=1)\n",
    "accuracy, accuracy_update_op = tf.metrics.accuracy(labels=y_labels, predictions=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calculate the loss and training operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001/3\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    iterator.initializer.run(feed_dict={X_train_ph:embedding_array_train, y_train_ph:y_train})\n",
    "    tf.local_variables_initializer().run()\n",
    "    while True:\n",
    "        try:\n",
    "            X_values, y_values = sess.run(next_element)\n",
    "            train_acc, _, _ = sess.run([accuracy, accuracy_update_op, training_op], feed_dict={X: X_values, y:y_values})\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "    ## Testing accuracy:\n",
    "    test_logits = sess.run(logits, feed_dict={X: embedding_array_test})\n",
    "    exp_logits = np.exp(test_logits)\n",
    "    y_test_pred_prob = exp_logits / exp_logits.sum(axis=1).reshape(-1,1)\n",
    "    y_test_pred = y_test_pred_prob.argmax(axis=1)\n",
    "    test_acc = (y_test_pred == y_test.argmax(axis=1)).mean()\n",
    "    return train_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train accuracy: 0.708, Test accuracy: 0.736\n",
      "Epoch: 2, Train accuracy: 0.755, Test accuracy: 0.755\n",
      "Epoch: 3, Train accuracy: 0.775, Test accuracy: 0.765\n",
      "Epoch: 4, Train accuracy: 0.790, Test accuracy: 0.778\n",
      "Epoch: 5, Train accuracy: 0.804, Test accuracy: 0.782\n",
      "Epoch: 6, Train accuracy: 0.817, Test accuracy: 0.790\n",
      "Epoch: 7, Train accuracy: 0.829, Test accuracy: 0.798\n",
      "Epoch: 8, Train accuracy: 0.839, Test accuracy: 0.800\n",
      "Epoch: 9, Train accuracy: 0.848, Test accuracy: 0.803\n",
      "Epoch: 10, Train accuracy: 0.856, Test accuracy: 0.804\n",
      "Epoch: 11, Train accuracy: 0.863, Test accuracy: 0.808\n",
      "Epoch: 12, Train accuracy: 0.869, Test accuracy: 0.807\n",
      "Epoch: 13, Train accuracy: 0.875, Test accuracy: 0.810\n",
      "Epoch: 14, Train accuracy: 0.880, Test accuracy: 0.809\n",
      "Epoch: 15, Train accuracy: 0.884, Test accuracy: 0.810\n",
      "Done training\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 15\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(n_epochs):\n",
    "        train_acc, test_acc = train_model()\n",
    "        print('Epoch: {}, Train accuracy: {:0.3f}, Test accuracy: {:0.3f}'.format(epoch+1, train_acc, test_acc))\n",
    "    print(\"Done training\")\n",
    "    save_path = saver.save(sess, \"./SavedModels/reviews-rnn.ckpt\")\n",
    "    test_pred = sess.run(y_pred, feed_dict={X: embedding_array_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  0\n",
      "Observed:  2\n",
      "my kids always want bourbon chicken when we go to food courts so i decided to do some research on recipes and found this item along with bourbon chicken marinade in the same line this sauce didnt disappoint us it tastes just like what we had at food courtsbr br "
     ]
    }
   ],
   "source": [
    "incorrect = (test_pred != y_test.argmax(axis=1))\n",
    "rev_index = 0\n",
    "print(\"Prediction: \", test_pred[incorrect][rev_index])\n",
    "print(\"Observed: \", y_test.argmax(axis=1)[incorrect][rev_index])    \n",
    "for x in X_test[incorrect][0]:\n",
    "    print(x, end=' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
